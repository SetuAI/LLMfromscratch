{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('the-verdict.txt','r',encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "\n",
    "print(\"Total characters in the text: \", len(raw_text))\n",
    "print(raw_text[:99])\n",
    "\n",
    "'''\n",
    "Our goal is to tokenizer 20479 characters into indiviual words \n",
    "that we can later turn into embeddings for LLM  training.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Note that it's common to process millions of articles and hundreds of thousands of books -- many gigabytes of text -- when working with LLMs. However, for educational purposes, it's sufficient to work with smaller text samples like a single book to illustrate the main ideas behind the text processing steps and to make it possible to run it in reasonable time on consumer hardware.\n",
    "\n",
    "How can we best split this text to obtain a list of tokens? \n",
    "For this, we go on a small excursion and use Python's regular expression library re for illustration purposes. (Note that you don't have to learn or memorize any regular expression syntax since we will transition to a pre-built tokenizer later in this chapter.)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tokens\n",
    "\n",
    "\n",
    "import re \n",
    "# regular expression matching expression\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text) \n",
    "#\\s is a whitespace character , it will split the text into indiviual words\n",
    "# where there are whitespaces\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n",
    "# The result is a list of individual words, whitespaces, and punctuation characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's modify the regular expression splits on whitespaces (\\s) and commas, \n",
    "# # and periods ([,.]):\n",
    "\n",
    "\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)\n",
    "print(len(result))\n",
    "\n",
    "\n",
    "'''now , and . are also split into separate tokens.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the whitespace characters are also included in the list of tokens.\n",
    "we can remove these redundant token.'''\n",
    "\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVING WHITESPACES OR NOT\n",
    "\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code is sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme that includes whitespaces.\n",
    "\n",
    "The tokenization scheme we devised above works well on the simple sample text. Let's modify it a bit further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double-dashes we have seen earlier in the first 100 characters of Edith Wharton's short story, along with additional special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Currently  only . and , are split into separate tokens.\n",
    "What if we also want ? , ! etc as separate tokens ? \n",
    "We need to include that in tokenization command using regular expression.\n",
    "'''\n",
    "\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "print(len(text))\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)\n",
    "print(len(result))\n",
    "\n",
    "'''\n",
    "If you look closely, we built the first tokenizer in these 2 lines of code : \n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "\n",
    "1. We split the text into tokens using a regular expression that matches whitespaces, commas, \n",
    "periods, colons, semicolons, question marks, underscores, exclamation marks, \n",
    "double quotes, parentheses, and hyphens.\n",
    "2. We remove any whitespace characters from the tokens.\n",
    "\n",
    "For building another tokenization technique is preferred like Byte Pair Encoding (BPE) or WordPiece Tokenization.\n",
    "'''\n",
    "\n",
    "# the standard length of the text is 31 characters\n",
    "# after removing whitespaces and splitting the text into tokens, we have 10 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to count whitespace characters in the text, we can use the following code:\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "whitespace_count = text.count(' ')\n",
    "print(f\"Number of whitespaces: {whitespace_count}\")  # Output: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we will apply these 2 lines \n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "\n",
    "on the entire raw text , input data of the book.\n",
    "\n",
    "'''\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text) # list of all tokens \n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()] # getting rid of whitespaces\n",
    "print(len(preprocessed)) # length of original text : 4690 tokens (-- , , are also counted as tokens)\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Tokens into Token IDs\n",
    "\n",
    "'''\n",
    "Once tokens are created, now we need to assign token IDs.\n",
    "But before token IDs, we need to create a vocabulary of all unique tokens.\n",
    "\n",
    "Now vocabulary is a set or a list of all unique tokens in the text.\n",
    "These tokens are first aligned/sorted alphabetically from a-z.\n",
    "And then , each unique token is mapped to a unique integer which is referred as token ID .\n",
    "\n",
    "For example,\n",
    "\n",
    "Input Data : My name is Chirantan Lonkar\n",
    "\n",
    "Vocabulary : \n",
    "\n",
    "Chirantan   0\n",
    "is          1\n",
    "Lonkar      2\n",
    "My          3\n",
    "name        4\n",
    "\n",
    "\n",
    "PS :\n",
    "VOCABULARY ONLY CONTAINS UNIQUE TOKENS.\n",
    "\n",
    "IF THE TOKEN \"THE\" APPEARS TWICE OR THRICE, IT ONLY APPEARS ONCE IN THE VOCABULARY.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the previous section, we tokenized Edith Wharton's short story and assigned it to a Python variable \n",
    "# called preprocessed. Let's now create a list of all unique tokens \n",
    "# and sort them alphabetically to determine the vocabulary size:\n",
    "\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "# vocabulary only contains unique words, hence the size is less than the total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After determining that the vocabulary size is 1,130 via the above code, \n",
    "we create the vocabulary and print its first 51 entries for illustration purposes:\n",
    "'''\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    '''enumerate is used to get the index of the item in the list, which act as token IDs here..'''\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n",
    "    \n",
    "# ! , , , etc are all assigned tokens on first priority basis.\n",
    "# post that , you can see all the tokens arranged in alphabetical order.\n",
    "\n",
    "# PS:\n",
    "# THIS PROCESS CAN ALSO BE THOUGHT OF AS ENCODING (TOKENS INTO TOKEN IDS)\n",
    "# WHEN USING DECODER WHICH MEANS FROM THE TOKEN ID , CONVERT TOKEN ID BACK TO THE WORD (reverse mapping process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, based on the output above, the dictionary contains individual tokens associated with unique integer labels.\n",
    "\n",
    "Later, when we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs into text.\n",
    "\n",
    "For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
    "\n",
    "Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary.\n",
    "\n",
    "In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text.\n",
    "\n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "\n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    '''Tokenizer class to encode and decode text and it takes vocabulary as input''' \n",
    "    def __init__(self,vocab):\n",
    "        \n",
    "        '''When you create an instance of tokenizer class, you have to create vocabulary. \n",
    "        Vocabulary is mapping from tokens to token IDs\n",
    "        Vocabulary = {token:integer for integer,token in enumerate(all_words)}\n",
    "        '''\n",
    "        self.str_to_int = vocab # mapping from token(str) to token ID(int)\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} # reverse mapping from token ID to token\n",
    "        # above code, consider s : token , i : token ID\n",
    "        \n",
    "        \n",
    "        # define Encoder and Decoder class\n",
    "        \n",
    "    def encode(self, text):\n",
    "        '''\n",
    "        Sample input text -> Tokens -> Token IDs\n",
    "        '''\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # take indiviual tokens\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() # get rid of whitespaces\n",
    "        ] \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    \n",
    "    def decode(self, ids):\n",
    "        '''Reverse dict : int to str : token ID to token , then convert token IDs into indiviual tokens\n",
    "          and then join indiviual tokens together\n",
    "          \n",
    "          Token IDs -> Tokens -> Sample input text\n",
    "          '''\n",
    "    \n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations , for example : The dog chased .  -> The dog chased.\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class \n",
    "and tokenize a passage from Edith Wharton's short story to try it out in practice:\n",
    "'''\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "           \n",
    "# testing encode method : take in text and convert it into token IDs\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(ids)\n",
    "\n",
    "# result : token IDS of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now we can decode the token IDs back into text using the decode method:\n",
    "it takes IDs as the input and returns the original text'''\n",
    "\n",
    "tokens = tokenizer.decode(ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
    "Based on the output above, we can see that the decode method successfully converted the token IDs back into the original text.\n",
    "\n",
    "So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing text based on a snippet from the training set.\n",
    "\n",
    "Let's now apply it to a new text sample that is not contained in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this sentence was not present in the vocabulary\n",
    "# may be hello is not there in the vocab \n",
    "# we get an error here.\n",
    "\n",
    "\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the word \"Hello\" was not used in the The Verdict short story.\n",
    "\n",
    "Hence, it is not contained in the vocabulary.\n",
    "\n",
    "This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "For example, ChatGPT has something called \"Special Context Tokens\"\n",
    "\n",
    "ADDING SPECIAL CONTEXT TOKENS\n",
    "In the previous section, we implemented a simple tokenizer and applied it to a passage from the training set.\n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown words.\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and <|endoftext|>\n",
    "\n",
    "PS :\n",
    "We can modify the tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\n",
    "\n",
    "Furthermore, we add a token between unrelated texts.\n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Adding Special Context Tokens:\n",
    "\n",
    "1. <|unk|> : unknown token\n",
    "2. <|endoftext|> : end of the text token\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------\n",
    "1.\n",
    "\n",
    "For example, take a sentence and tokenize it\n",
    "\n",
    "Input Sentence : The fox chased the dog\n",
    "\n",
    "Tokenized : \"The\" \"fox\" \"chased\" \"the\" \"dog\"\n",
    "\n",
    "Existing Vocabulary (arranged in aplhabetical order with token IDs) :\n",
    "\n",
    "\"chased\" 0\n",
    "\"dog\" 1\n",
    "\"fox\" 2\n",
    "\"the\" 3\n",
    "\n",
    "PS : Now to this existing vocabulary we will add 2 more tokens and assign them Token IDs : <|unk|> :783 & <|endoftext|> :784 \n",
    "\n",
    "They are the last 2 tokens in the vocabulary.\n",
    "\n",
    "Now let's say that we add an UNKNOWN word called \"quickly\" to the input sentence.\n",
    "\n",
    "Input Sentence : The fox chased the dog quickly.\n",
    "\n",
    "For we have token IDs for all the tokens except \"quickly\" as it is not present in the vocabulary.\n",
    "In that case, we will assign the Token ID : 783 to the token \"quickly\" as it is unknown. Which is also what |<unk>| token represents.\n",
    "\n",
    "|<unk>| token is used to represent unknown tokens in the vocabulary and assign them respective token IDs.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------\n",
    "2. \n",
    "<|endoftext|> token is usually added at the end of the text to indicate the end of the text between 2 text sources.\n",
    "\n",
    "What does this do ?\n",
    "\n",
    "Say for example you have 4 text documents \n",
    "\n",
    "Text 1 : Book \n",
    "Text 2 : Newspaper Article\n",
    "Text 3 : Research Paper\n",
    "Text 4 : Blog Post\n",
    "\n",
    "When you are training a model , and you need pass the corpus , all these sentences are not stacked up together and collated.\n",
    "\n",
    "<|endoftext|> token is an indicator that the Text1 has ended and Text 2 starts.\n",
    "\n",
    "So this token , is added between acting as marker : signalling as start or end of the text.\n",
    "\n",
    "\n",
    "If <|endoftext|> was not there, the LLM would have mixed all of these together, this helps data to be processed better.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"]) # add 2 more tokens using extend method in lists\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab.items())\n",
    "\n",
    "# the original size of the vocabulary was 1,130 tokens, and now it's 1,132 token \n",
    "# as we added 2 more tokens : |<unk>| and |<endoftext>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an additional quick check, let's print the last 5 entries of the updated vocabulary:\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    '''Creating Tokenizer version 2'''\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        '''first 2 lines of code are the same\n",
    "        if the particular entry is not present in the vocab assign it to |<unk>| token ID\n",
    "        '''\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed] # encoding the text\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        '''Nothing changes from Version 1 , we convert Token ID back to tokens and then join them together'''\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying version 2 of the tokenizer on the same text\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(text)\n",
    "# you can see Token ID for \"Hello\" is 1131 as Hello is not present in the main vocabulary\n",
    "# Moreover, the main vocabulary had 1130 characters \n",
    "# So at 1130 the text ended , hence represented by |<endoftext>| token ID\n",
    "# 1131 was reserved for |<unk>| token ID and because Hello was not present in the main vaocabulary it was asssigned 1131."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will use deocde function\n",
    "# we will pass the encoded text (token IDs)\n",
    "# into the tokenizer.decode\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "# You can see \"Hello\" and \"palace\" were not present in the vocabulary , so token IDs for both are 1131.\n",
    "\n",
    "# This means we are able to handle the unkwown words effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'\n",
    "Based on comparing the de-tokenized text above with the original input text, we know that the training dataset, Edith Wharton's short story The Verdict, did not contain the words \"Hello\" and \"palace.\"\n",
    "\n",
    "So far, we have discussed tokenization as an essential step in processing text as input to LLMs. Depending on the LLM, some researchers also consider additional special tokens such as the following:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks the start of a text. It signifies to the LLM where a piece of content begins. Opposite of end of text\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text, and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one article ends and the next one begins.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token, up to the length of the longest text in the batch.\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned above but only uses an <|endoftext|> token for simplicity\n",
    "\n",
    "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words.\n",
    "\n",
    "Instead, GPT models use a byte pair encoding tokenizer, which breaks down words into subword units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
