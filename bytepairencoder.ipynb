{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte-Pair Encoding\n",
    "\n",
    "Behind GPT-2, GPT-3 or modern LLMs, the tokenizer they use is usually Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of Tokenizer \n",
    "\n",
    "1. Word Based Tokenizer  : (Here, every word in the sentence is 1 token)\n",
    "For example :   Fox chased the dog :\n",
    "Output Tokens : \"Fox\", \"chased\", \"the\", \"dog\"\n",
    "\n",
    "Problems with Word-Based Tokenizer : \n",
    "\n",
    "- what will you do with words which are not present in the vocabulary ? \n",
    "- These words are also called as OOV (Out-Of-Vocabulary words)\n",
    "- For example, if we add 2 more words : and , monkey\n",
    "So \"and\" and \"monkey\" are 2 new added words and what if they are not there in vocab\n",
    "Then usually the word-based tokenizer runs into an error.\n",
    "\n",
    "- In Word Based tokenizer, similar words like \"boy\" and \"boys\" will be treated as seperate \n",
    "tokens\n",
    "- Hence it becomes difficult to capture the similarity\n",
    "----------------------------------------------------------------------------------------\n",
    "\n",
    "2. Sub-Word Based\n",
    "\n",
    "Byte pair encoding is an example of Sub-Word Based tokenization.\n",
    "It is the best of both worlds.\n",
    "\n",
    "It does capture some root words :\n",
    "For example , \"boy\" and \"boys\" will not be treated as seperate token\n",
    "It will treat boy as common root word.\n",
    "\n",
    "Sub-Word Based Tokenization follows two rules :\n",
    "\n",
    "Rule 1 : Do not split frequently used words into smaller sub words.\n",
    "If the word \"boy\" is appearing multiple times in the dataset, it should not be split\n",
    "further.\n",
    "boy is retained as a token.\n",
    "\n",
    "\n",
    "Rule 2: Split the rare words into smaller and meaningful sub words. (IMPORTANT)\n",
    "If needed can be also dropped down into character level.\n",
    "\n",
    "For example ,boy is appearing mutliple times .\n",
    "But the term \"boys\" occurs rarely, hence break it down as \"boy\" and \"s\".\n",
    "Which is Word based token and character based token.\n",
    "\n",
    "\n",
    "Advantages :\n",
    "\n",
    "Subword helps model learn different words with same root words effectively.\n",
    "\n",
    "For example, \"token\" , \"tokens\" and \"tokenizing\" are similar in meaning and have \n",
    "same root word . \n",
    "This meaning is lost in word based and character based tokenization.\n",
    "\n",
    "It also helps the model learn nuances.\n",
    "For example, words like \"tokenization\" and \"modernization\" \n",
    "They both have commmon suffix \"ization\".\n",
    "The model will learn that though both words are made up of different root words\n",
    "i.e. \"token\" and \"modern\" , they both have common suffix \"ization\" and are used in \n",
    "same syntactic situations.\n",
    "\n",
    "\n",
    "BPE is a sub-word tokenization algorithm.\n",
    "It was introduced in 1994.\n",
    "It is basically a data-compression algorithm.\n",
    "\n",
    "BPE paper : https://arxiv.org/pdf/1209.1045\n",
    "\n",
    "Example Explanation of Byte-Pair Encoding :\n",
    "------------------------------------------------------------------------------------\n",
    "Let's understand with an example\n",
    "\n",
    "Original data looks like : aaabdaaabac\n",
    "\n",
    "Now we can notice, the most common byte-pair is \"aa\" .(occurs consecutively)\n",
    "(aaa also appears twice, but BPE algorithm looks for a \"pair\" to be encoded)\n",
    "\n",
    "Now, we replace this byte-pair (\"aa\") with a byte that DOES NOT EXIST in the data\n",
    "For example : Z\n",
    "Now wherever \"aa\" appears, we replace it with \"Z\" .\n",
    "Hence, the ouput will look like : ZabdZabac (this is the compressed version of original version)\n",
    "\n",
    "Now , next we keep on repeating this sequentially and find the next\n",
    "common consecutive byte-pair.\n",
    "\n",
    "2nd iteration : Compressed Data : ZabdZabac\n",
    "Now we can see, the pair \"ab\" is occuring twice consecutively.\n",
    "We will replace this with Y (a byte that does not exist in the data).\n",
    "\n",
    "Now the output looks like : ZYdZYac\n",
    "\n",
    "We keep on doing this recursively again.\n",
    "\n",
    "The end pair \"ac\" on the right, appears only once, hence no need to encode it.\n",
    "This process of replacing common byte-pairs with another variables is called\n",
    "byte-pair encoding.\n",
    "Now further , you can again take the compressed data : ZYdZYac\n",
    "One more layer deeper : replacing \"ZY\" with say \"W\"\n",
    "Then output will look like : WdWac  and then you cannot compressed it further.\n",
    "\n",
    "So you can see the original data : aaabdaaabac\n",
    "Has been compressed to : WdWac \n",
    "\n",
    "That's Byte-Pair Encoding Algorithm in a nutshell .\n",
    "----------------------------------------------------------------------------------\n",
    "\n",
    "How is BPE used in LLMs ? \n",
    "\n",
    "- BPE ensures most common words in the vocabulary are represented as single token(Rule \n",
    "- Rare words are broken down into one or more sub tokens\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "\n",
    "3. Character Based :\n",
    "If the sentence is : \n",
    "My hobby is playing cricket .\n",
    "\n",
    "Here indiviual characters are considered as tokens : \n",
    "\n",
    "[\"m\",\"y\",\"h\",\"o\",\"b\",\"b\",....]\n",
    "\n",
    "Now in this case ? What will be the size of vocabulary ? \n",
    "\n",
    "Too small . It will be just 256 characters in english language.\n",
    "English language has in total around 170k to 200k words.\n",
    "\n",
    "Advantages : \n",
    "It has a small vocabulary size, so no issue of Out of vocabulary problem.\n",
    "It will be always either out of the 256 characters.\n",
    "\n",
    "256 characters breakdown : \n",
    "26 uppercase letters (A-Z)\n",
    "26 lowercase letters (a-z)\n",
    "10 numerical digits (0-9)\n",
    "Common punctuation marks (~`!@#$%^&*()_+-={}[]|\":;'<>,.?/)\n",
    "Space character\n",
    "\n",
    "\n",
    "Problems : \n",
    "The meaning associated with the word is completely lost.\n",
    "Different words and sentences have meanings which is lost.\n",
    "Also the tokenized sequence is much longer than the raw text.\n",
    "\n",
    "For example :\n",
    "If the sentence contains the word \"Dinousaur\" and if you apply \n",
    "Word Based Tokenizer, then \"Dinousaur\" will be treated as one single token.\n",
    "\n",
    "In Character Based Tokenizer it will be split into 8 tokens.\n",
    "\"D\", \"i\" ,...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
